{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import csv\n",
    "import translators as ts\n",
    "import translators.server as tss\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset processing - Create the csv\n",
    "if os.getcwd().find(\"Dataset\") == -1:\n",
    "    os.chdir(\"./Dataset/\")\n",
    "dataset = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv files\n",
    "def createCSV():\n",
    "    for file in dataset:\n",
    "        fileName = file[:file.find(\".\")]+\".csv\" \n",
    "        with open(file, \"r\", encoding=\"utf-8\",newline='') as file, open(fileName, 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            for idx, line in enumerate(file):\n",
    "                if idx == 0:\n",
    "                    fields = [\"word\",\"accuracy\"]\n",
    "                else:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the files .csv and format to be word,emotion,accuracy\n",
    "if os.getcwd().find(\"Dataset\") == -1:\n",
    "    os.chdir(\"./Dataset/\")\n",
    "dataset = os.listdir()\n",
    "\n",
    "def listToString(s):\n",
    "    # initialize an empty string\n",
    "    str1 = \"\\n\"\n",
    "    # return string \n",
    "    return (str1.join(s))\n",
    "\n",
    "def translateFile(file, output):\n",
    "    from_language, to_language = 'en', 'pt'\n",
    "    pos = file.find(\".csv\")\n",
    "    emotion = file[:pos]\n",
    "    with open(file,\"r\", encoding=\"utf-8\", newline='') as file:\n",
    "        words = []\n",
    "        data = []\n",
    "        accuracies = []\n",
    "        emotions = []\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            fields = line.strip().split(',')\n",
    "            accuracies.append(fields[1])\n",
    "            words.append(fields[0])\n",
    "            emotions.append(emotion)\n",
    "    for i in range(0, len(words), 400):     \n",
    "        block = words[i:i+400]\n",
    "        block_size = len(block)  \n",
    "        if (i + block_size) >= len(words):\n",
    "            block = words[i:(len(words)-1)]\n",
    "            break\n",
    "        translated_text = tss.google(listToString(block), reset_host_url=None, from_language=from_language, to_language=to_language)\n",
    "        for i, word in enumerate(translated_text.split(\"\\n\")):\n",
    "            if i != 0:\n",
    "                data.append(word[1:len(word)-1])\n",
    "            else:    \n",
    "                data.append(word[:len(word)-1])    \n",
    "    with open(output, 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"word\",\"emotion\",\"accuracy\"])\n",
    "        for i, item in enumerate(data):\n",
    "            writer.writerow([item,emotions[i],accuracies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate files\n",
    "translateFile(\"anger.csv\", \"anger_PT.csv\")\n",
    "translateFile(\"antecipation.csv\", \"antecipation_PT.csv\")\n",
    "translateFile(\"disgust.csv\", \"disgust_PT.csv\")\n",
    "translateFile(\"fear.csv\", \"fear_PT.csv\")\n",
    "translateFile(\"joy.csv\", \"joy_PT.csv\")\n",
    "translateFile(\"sadness.csv\", \"sadness_PT.csv\")\n",
    "translateFile(\"surprise.csv\", \"surprise_PT.csv\")\n",
    "translateFile(\"trust.csv\", \"trust_PT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename files\n",
    "dataset = os.listdir()\n",
    "for file in dataset:\n",
    "    pos = file.find(\"_PT\")\n",
    "    if pos != -1:\n",
    "       os.rename(file,file[:pos]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "mergedCSV = []\n",
    "headers = True\n",
    "dataset = os.listdir()\n",
    "totalLines = 0\n",
    "for file in dataset:\n",
    "    pos = file.find(\".csv\") \n",
    "    if file.find(\".csv\") != -1:\n",
    "        \n",
    "        name = file[:pos]\n",
    "        with open(file, 'r',encoding=\"utf-8\") as fileData:\n",
    "            reader = csv.reader(fileData)\n",
    "            csvData = list(reader) \n",
    "            totalLines = totalLines + len(csvData)\n",
    "            if headers:\n",
    "                headers = False\n",
    "            else:\n",
    "                csvData = csvData[1:]  \n",
    "            mergedCSV = mergedCSV + csvData\n",
    "with open('dataset.csv', 'w', newline='',encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(mergedCSV)\n",
    "    print(len(mergedCSV),totalLines)  \n",
    "data.to_csv('dataFinal.csv', index=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataFinal.csv\",sep=\",\")\n",
    "# Preprocess the text\n",
    "# Tokenization, stemming, stop-word removal, etc.\n",
    "\n",
    "# Feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.2, max_features=1000)\n",
    "\n",
    "X = []\n",
    "for word in data['word']:\n",
    "    X.append(str(word))\n",
    "y = data['emotion']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data into feature vectors\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# Train the SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set \n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4724978 0.5275023]\n",
      "dict_values(['negative', 'positive'])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"Eu amo essa cidade. Ela Ã© linda e cheia de vida.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "filtered_tokens = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(' '.join(filtered_tokens))\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
