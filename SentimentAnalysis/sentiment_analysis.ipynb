{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\ER_NUI\\SentimentAnalysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from deep_translator import MyMemoryTranslator\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/PoorvaRane/Emotion-Detector/blob/master/ISEAR.csv\n",
    "#https://github.com/sinmaniphel/py_isear_dataset/blob/master/isear.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data():\n",
    "    with open(\"data.csv\", \"r\", ) as input_file, open(\"transformed_data.csv\", \"w\") as output_file:\n",
    "        for line in input_file:\n",
    "            cleaned = line.strip()\n",
    "            pos = cleaned.find(',')\n",
    "            emotion = cleaned[:pos]\n",
    "            # replace the first , after emotion\n",
    "            if emotion in [\"joy\",\"fear\",\"anger\",\"sadness\",\"disgust\",\"shame\",\"guilt\"]:\n",
    "                cleaned = '%s%s%s'%(cleaned[:pos],\";\",cleaned[pos+1:])\n",
    "            print(cleaned,len(cleaned))\n",
    "            if cleaned[len(cleaned)-1] == \",\":\n",
    "                cleaned = '%s%s%s'%(cleaned[:len(cleaned)-1],\";\",cleaned[len(cleaned):])\n",
    "            \n",
    "            output_file.write(f'{cleaned}\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transformed_data.csv', 'r') as f, open(\"transformed_data_text.csv\", \"w\") as output_file:\n",
    "    reader = csv.reader(f, delimiter=';')\n",
    "    for row in reader:\n",
    "        #print(row)\n",
    "        line=row[1].replace(\"\\n\",\" \")\n",
    "        emo = row[0]\n",
    "        output_file.write(f'{emo};{line}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_language, to_language = 'en', 'pt-PT'\n",
    "with  open(\"ISEAR_2_formatted.csv\", \"r\") as file, open(\"ISEAR_portuguese_2.csv\", \"a\", encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    reader2 = csv.reader(output_file, delimiter=';')\n",
    "    size = 0\n",
    "    for idx, row in tqdm(enumerate(reader)):\n",
    "        if idx < size:\n",
    "            continue\n",
    "        else:\n",
    "            input_text = row[1]\n",
    "            #translated_text = tss.google(input_text, reset_host_url=None, from_language=from_language, to_language=to_language)\n",
    "            #translated_text = input_text.translate(to='pt-PT', from_lang='en')\n",
    "            translated_text = MyMemoryTranslator(source='english', target='portuguese').translate(input_text) \n",
    "\n",
    "            output_file.write(f'{row[0]};{translated_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"ISEAR_portuguese.csv\",sep=\";\")\n",
    "texts = dataset[\"text\"].values\n",
    "classes = dataset[\"sentiment\"].values\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "tokenizer = Tokenizer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, classes, test_size=0.03, random_state=2)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "seq_lengths = [len(seq.split()) for seq in X_train]\n",
    "max_len = int(np.percentile(seq_lengths, 95))\n",
    "print(vocab_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                             max_features = 10000, stop_words=stop_words)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(X_train)\n",
    "print(\"########## Dictionary built ##########\\n\")\n",
    "test_data_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data_features, y_train)\n",
    "MNBpredict = model.predict(test_data_features)\n",
    "print(\"Accuracy: {0:.2f}%\".format(metrics.accuracy_score(y_test, MNBpredict)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "X_test = vectorizer.transform([\"Estou triste\"])\n",
    "y_pred = model.predict_proba(X_test)\n",
    "print(y_pred, model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_len))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"ISEAR_2_formatted.csv\",sep=\";\")\n",
    "sentiments = dataset[\"sentiment\"].values\n",
    "with open(\"ISEAR_only_text_eng.csv\", \"r\", encoding=\"utf-8\") as file, open(\"ISEAR_only_text_eng_final.csv\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    reader = csv.reader(file, delimiter='\\n')\n",
    "    for idx,row in enumerate(reader):\n",
    "        output_file.write(f'{sentiments[idx]};{row[0]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
